# -*- coding: utf-8 -*-
"""Copy of Data cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LaXEgBW9orTVa75VwvCjGIKdWRih63SR
"""

from google.colab import files
import pandas as pd

uploaded = files.upload()
df = pd.read_csv("heart.csv")

# Display basic info
print(df.info())
print(df.head())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df.shape

print(df.dtypes)

print(df.info())

# Check for missing values
print(df.isnull().sum())

"""**Exploratory Data Analysis**"""

sns.countplot(x='target', data=df)
plt.title('Target Variable Distribution')
plt.xlabel('Heart Disease (1 = Yes, 0 = No)')
plt.ylabel('Count')
plt.show()

# Print counts
print(df['target'].value_counts())

plt.figure(figsize=(14,10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix")
plt.show()

num_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

for col in num_features:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

from matplotlib import pyplot as plt
import seaborn as sns

plt.figure(figsize=(14,8))
sns.boxplot(df)
plt.show()

cat_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

for col in cat_features:
    plt.figure(figsize=(6, 4))
    sns.countplot(x=col, hue='target', data=df)
    plt.title(f'{col} vs Target')
    plt.legend(title='Heart Disease')
    plt.show()

# This takes time but shows relationships well
sns.pairplot(df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'target']], hue='target')
plt.show()

categorical_cols = ['cp', 'thal', 'slope']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
df

"""The countplot illustrates that the proportion of individuals with heart disease is almost identical between smokers and non-smokers. Based on the normalized proportions, approximately 20% of both smokers and non-smokers have heart disease, while around 80% do not. This indicates that smoking, on its own, may not be a strong predictor of heart disease in this dataset."""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Features and target
X = df_encoded.drop('target', axis=1)
y = df_encoded['target']

# Split the data first
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""##Model BUilding"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'SVM': SVC()
}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    print(f"\nüìå {name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.model_selection import cross_val_score

for name, model in models.items():
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
    print(f"{name} Cross-Val Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})")

import matplotlib.pyplot as plt

importances = models['Random Forest'].feature_importances_
features = X.columns

# Plot
plt.figure(figsize=(10,6))
sns.barplot(x=importances, y=features)
plt.title("Feature Importance - Random Forest")
plt.show()

import joblib

joblib.dump(scaler, 'scaler.pkl')


# Use the DataFrame version (not scaled) to keep feature names
best_model = RandomForestClassifier()
best_model.fit(X_train, y_train)

# Save it again
joblib.dump(best_model, 'heart_disease_model.pkl')

"""‚úÖ SVM is performing well:
High accuracy (0.87)

Balanced precision/recall

Strong at catching true positives (recall = 0.93)

‚ö†Ô∏è Random Forest has 100% accuracy, which usually means:
Overfitting: It's too perfect, likely memorizing the training data or test set due to low data size or leakage.

Check if the test set was completely unseen during training.

Double-check if scaling or label leakage occurred.

‚úÖ KNN and Logistic Regression:
Perform decently with 82‚Äì83% accuracy.

Logistic Regression shows slight class imbalance in precision/recall.
"""

from sklearn.model_selection import cross_val_score
import numpy as np

# Models dictionary (reuse scaled features from before)
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'SVM': SVC()
}

print("üìä 5-Fold Cross-Validation Results:\n")
for name, model in models.items():
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
    print(f"{name}:")
    print(f"  Mean Accuracy: {scores.mean():.4f}")
    print(f"  Standard Deviation: {scores.std():.4f}")
    print(f"  All Fold Scores: {scores}\n")

import pandas as pd

results = {
    'Model': [],
    'Mean Accuracy': [],
    'Std Dev': [],
    'Fold Scores': []
}

for name, model in models.items():
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
    results['Model'].append(name)
    results['Mean Accuracy'].append(scores.mean())
    results['Std Dev'].append(scores.std())
    results['Fold Scores'].append(scores)

results_df = pd.DataFrame(results)
print(results_df)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.barplot(x='Mean Accuracy', y='Model', data=results_df.sort_values('Mean Accuracy', ascending=False), palette='viridis')
plt.title('Model Comparison (5-Fold CV Accuracy)')
plt.xlabel('Mean Accuracy')
plt.ylabel('Model')
plt.xlim(0.8, 1.0)
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the parameter grid
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'bootstrap': [True, False]
}

# Set up GridSearchCV
rf_grid = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=rf_params,
    cv=5,
    n_jobs=-1,
    scoring='accuracy',
    verbose=1
)

# Fit the model
rf_grid.fit(X_train_scaled, y_train)

# Best parameters and score
print("üå≤ Best Random Forest Parameters:", rf_grid.best_params_)
print("‚úÖ Best Random Forest Accuracy:", rf_grid.best_score_)

from sklearn.svm import SVC

# Define parameter grid
svm_params = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# Set up GridSearchCV
svm_grid = GridSearchCV(
    estimator=SVC(),
    param_grid=svm_params,
    cv=5,
    n_jobs=-1,
    scoring='accuracy',
    verbose=1
)

# Fit the model
svm_grid.fit(X_train_scaled, y_train)

# Best parameters and score
print("üîç Best SVM Parameters:", svm_grid.best_params_)
print("‚úÖ Best SVM Accuracy:", svm_grid.best_score_)

# Use best estimators directly
rf_best = rf_grid.best_estimator_
svm_best = svm_grid.best_estimator_

# Predict
rf_pred = rf_best.predict(X_test_scaled)
svm_pred = svm_best.predict(X_test_scaled)

# Evaluate
from sklearn.metrics import accuracy_score, classification_report

print("üéØ Random Forest Test Accuracy:", accuracy_score(y_test, rf_pred))
print("Classification Report:\n", classification_report(y_test, rf_pred))

print("\nüéØ SVM Test Accuracy:", accuracy_score(y_test, svm_pred))
print("Classification Report:\n", classification_report(y_test, svm_pred))

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Fit all models again
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'SVM': SVC(probability=True)  # Enable probability estimates
}

# Train and store FPR, TPR, AUC for each model
plt.figure(figsize=(10, 7))

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_probs = model.predict_proba(X_test_scaled)[:, 1]  # get probabilities for class 1
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot formatting
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Heart Disease Prediction')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""##Feature Importance for Logistic Regression

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Train Logistic Regression
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_scaled, y_train)

# Get feature names (you might need to adjust this based on your variable names)
feature_names = X.columns

# Get coefficients
coefficients = logreg.coef_[0]

# Create a DataFrame for visualization
coef_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coefficients
})

# Sort by absolute value of coefficients
coef_df['abs_coef'] = coef_df['Coefficient'].abs()
coef_df = coef_df.sort_values(by='abs_coef', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette='coolwarm')
plt.title('Feature Importance in Logistic Regression')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib
joblib.dump(rf_best, 'best_random_forest_model.pkl')

from sklearn.preprocessing import PolynomialFeatures

# Polynomial Features
poly = PolynomialFeatures(degree=2, interaction_only=True)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

from scipy.stats import zscore
from sklearn.preprocessing import MinMaxScaler

# Remove Outliers using Z-score
X_train_filtered = X_train_scaled[(zscore(X_train_scaled) < 3).all(axis=1)]
y_train_filtered = y_train[(zscore(X_train_scaled) < 3).all(axis=1)]

# Min-Max Scaling
scaler = MinMaxScaler()
X_train_minmax = scaler.fit_transform(X_train_filtered)
X_test_minmax = scaler.transform(X_test_scaled)

from sklearn.ensemble import StackingClassifier

stacking_model = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(max_iter=1000)),
        ('rf', RandomForestClassifier(n_estimators=200)),
        ('svm', SVC(probability=True))
    ],
    final_estimator=RandomForestClassifier()
)

stacking_model.fit(X_train_scaled, y_train)
stacking_pred = stacking_model.predict(X_test_scaled)

print("‚úÖ Stacking Accuracy:", accuracy_score(y_test, stacking_pred))

from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

!pip install optuna

import optuna
from sklearn.model_selection import cross_val_score

def objective(trial):
    n_estimators = trial.suggest_int("n_estimators", 100, 300)
    max_depth = trial.suggest_int("max_depth", 5, 30)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 5)

    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf
    )

    score = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()
    return score

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)

print("üîç Best Parameters:", study.best_params)



from sklearn.metrics import roc_auc_score

rf_best = RandomForestClassifier(**study.best_params)
rf_best.fit(X_train_scaled, y_train)

y_prob = rf_best.predict_proba(X_test_scaled)[:,1]
print("üéØ Final ROC-AUC Score:", roc_auc_score(y_test, y_prob))

import joblib

joblib.dump(rf_best, 'optimized_random_forest.pkl')
print("‚úÖ Optimized Model saved as 'optimized_random_forest.pkl'")